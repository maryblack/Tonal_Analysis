{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "sVhijsSUtmft",
    "outputId": "1113b4c4-8b1d-4e36-ba54-a7703ef9b836"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "import io\n",
    "import os\n",
    "from getpass import getuser\n",
    "import re\n",
    "import math\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import pymorphy2\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cL2c8JddtoR_"
   },
   "outputs": [],
   "source": [
    "TEST_FILENAME = os.path.join('/home',getuser(), 'Tonal_Analysis', 'news_sentiment_romip2012-1/test/news_eval_test.xml')\n",
    "TRAIN_FILENAME = os.path.join('/home',getuser(), 'Tonal_Analysis', 'news_sentiment_romip2012-1/train/news_eval_train.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rdc6wFqkueuH"
   },
   "outputs": [],
   "source": [
    "class Citation:\n",
    "    def __init__(self, words, evaluation: str):\n",
    "        self.words = words\n",
    "        self.eval = evaluation\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.eval}: {self.words}'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J7SmS5GGue2y"
   },
   "outputs": [],
   "source": [
    "def tokenize(file_text, remove_words=False):\n",
    "    tokens = nltk.word_tokenize(file_text)\n",
    "    tokens = [i for i in tokens if ( i not in string.punctuation )]\n",
    "    stop_words = stopwords.words('russian')\n",
    "    if remove_words:\n",
    "        words_to_remove = [ 'все', 'нет', 'ни',  'ничего', 'без', 'никогда', 'наконец', 'больше', 'хорошо', 'лучше','нельзя', 'более', 'всегда', 'конечно', 'всю', 'такой', 'впрочем', 'так', 'вот', 'можно', 'даже', 'разве']\n",
    "        for word in words_to_remove:\n",
    "            stop_words.remove(word)\n",
    "    tokens = [morph.parse(re.sub(r'[^\\w\\s]', '', i).lower())[0].normal_form for i in tokens if ( i not in stop_words )]\n",
    "    tokens = [i.replace(\"«\", \"\").replace(\"»\", \"\") for i in tokens]\n",
    "    for item in tokens:\n",
    "        if '' == item or item.isspace():\n",
    "            while item in tokens:\n",
    "                tokens.remove(item)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "def parse_xml(file: str) -> list:\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "    corpus = []\n",
    "    citations = []\n",
    "    for elem in root.iter('speech'):\n",
    "        corpus.append(tokenize(elem.text))\n",
    "    i = 0\n",
    "    for elem in root.iter('evaluation'):\n",
    "        pair_eval = elem.text.replace(\"\\n\", \"\")\n",
    "        pair_eval = ''.join(pair_eval.split())\n",
    "        # print (corpus[i])\n",
    "        if pair_eval in ['0', '+', '-']:\n",
    "            citation = Citation(corpus[i], pair_eval)\n",
    "            citations.append(citation)\n",
    "\n",
    "        i += 1\n",
    "        # evaluate_id.append(elem.text)\n",
    "    return citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IBS54gZ1vfUr"
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "aZir63VHue07",
    "outputId": "b618f8d7-f0b4-4b20-c31d-eb8907474e8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.1 s, sys: 284 ms, total: 56.4 s\n",
      "Wall time: 56.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "citations_train = parse_xml(TRAIN_FILENAME)\n",
    "citations_test = parse_xml(TEST_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jydncBYtuexS"
   },
   "outputs": [],
   "source": [
    "def vectorized_corpus(parsed_list_train, parsed_list_test):\n",
    "    out_list_train = []\n",
    "    out_list_test = []\n",
    "    eval_list = []\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for cite in parsed_list_train:\n",
    "        citation = cite.words\n",
    "        evaluation = cite.eval\n",
    "        citation = ' '.join(citation).strip()\n",
    "        out_list_train.append(citation)\n",
    "        y_train.append(evaluation)\n",
    "        \n",
    "    for cite in parsed_list_test:\n",
    "        citation = cite.words\n",
    "        evaluation = cite.eval\n",
    "        citation = ' '.join(citation).strip()\n",
    "        out_list_test.append(citation)\n",
    "        y_test.append(evaluation)\n",
    "    \n",
    "    #vectorizer = CountVectorizer()#булевский\n",
    "    #vectorizer = HashingVectorizer(n_features=2**17)\n",
    "    vectorizer = TfidfVectorizer()#tf-idf\n",
    "    train_data = vectorizer.fit_transform(out_list_train)\n",
    "    X_train = train_data.toarray()\n",
    "#     print(out_list_test)\n",
    "    test_data = vectorizer.transform(out_list_test)\n",
    "    X_test = test_data.toarray()\n",
    "    return X_train, y_train, X_test, y_test, vectorizer\n",
    "\n",
    "X_train, y_train, X_test, y_test, vectorizer = vectorized_corpus(citations_train,citations_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7lMYfw0vn2E"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C=1.0, class_weight='balanced', dual=False, fit_intercept=True, tol=0.0001,\n",
    "         intercept_scaling=1, max_iter=100, penalty='l2', random_state=0, solver='saga', multi_class='ovr',  warm_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "EIL8vyfqvoKB",
    "outputId": "bcb6fc93-4b39-4c82-a9be-edc33ca5872f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(class_weight='balanced', multi_class='ovr', random_state=0,\n",
      "                   solver='saga', warm_start=True)\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fQjUwpNevoNq"
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "expected = y_test\n",
    "predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aznwGT3kwJry",
    "outputId": "e02f2ed3-aa42-4226-e099-c72c5b5eb84e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['+', '0', '0', ..., '+', '0', '-'], dtype='<U1')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "1fiJPE4Dswod",
    "outputId": "34afe8bb-ef47-4c1a-a357-1393d94d157a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           +       0.56      0.72      0.63      1448\n",
      "           -       0.71      0.66      0.69      1890\n",
      "           0       0.52      0.40      0.45      1235\n",
      "\n",
      "    accuracy                           0.61      4573\n",
      "   macro avg       0.60      0.59      0.59      4573\n",
      "weighted avg       0.61      0.61      0.60      4573\n",
      "\n",
      "0.6090094030177127\n"
     ]
    }
   ],
   "source": [
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(model.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lLGDHq9As72G"
   },
   "outputs": [],
   "source": [
    "example = \"бог\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iyj-hLPts70A"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['+'], dtype='<U1')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(vectorizer.transform([' '.join(tokenize(example))]).toarray())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tonality.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "common-cpu.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}